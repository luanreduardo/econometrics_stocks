---
title: "<center> <h2> <b> Trabalho </b> </h2> </center>"
author: "<center> Fernando A. B. Sabino da Silva - UFRGS </center>"
references:
- author:
  - family: Tsay
    given: Ruey S
  id: tsay2014introduction
  issued:
    year: 2014
  publisher: John Wiley \& Sons
  title: An introduction to analysis of financial data with R
  type: book
- author:
  - family: Pfaff
    given: Benhard
  id: pfaff2016
  issued:
    year: 2016
  publisher: John Wiley \& Sons
  title: Financial risk modelling and portfolio optimization with R
  type: book
- author:
  - family: Engle
    given: Robert F
  id: engle1982autoregressive
  issued:
    year: 1982
  publisher: Econometrica Journal of the Econometric Society
  title: Autoregressive conditional heteroscedasticity with estimates of the variance
    of United Kingdom inflation
  type: article
- author:
  - family: Dickey
    given: David A
  - family: Fuller
    given: Wayne A
  id: dickey1979distribution
  issue: 366a
  issued:
    year: 1979
  page: 427-431
  publisher: Journal of the American statistical association
  title: Distribution of the estimators for autoregressive time series with a unit
    root
  type: article-journal
  volume: 74
- author:
  - family: Bueno
    given: Rodrigo De Losso da Silveira
  id: delosso2012
  issued:
    year: 2012
  publisher: Cengage Learning
  title: Econometria de séries temporais
  type: book
- author:
  - family: Hafner
    given: C. M.
  id: hafner1998
  issued:
    year: 1998
  publisher: Journal of Statistical Planning and Inference
  title: Estimating high-frequency foreign exchange rate volatility with nonparametric ARCH models
  type: article
output:
  pdf_document: default
  html_document:
    df_print: paged
urlcolor: blue
---

Este trabalho tem como objetivo colocar em prática os modelos de séries temporais univariadas **AR(p)**, **MA(q)**, **ARMA(p,q)**, **ARCH(m)** e **GARCH(m,n)** em uma série temporal de retornos de um ativo financeiro. 

Suponha que $r_{t}$ é o logaritmo do retorno de um ativo financeiro em $t$  e que $\left\{r_{t}\right\}_{t=1}^{T}$ é a série temporal do logaritmo de $T$ retornos. Para modelar tal série temporal dentro das classes de modelos vistasno curso, nós temos duas equações que precisam ser estimadas: a média condicional e a variância condicional. Para a equação da média condicional, podemos assumir um modelo $ARMA(p,q)$, neste formato:

$$
\begin{aligned}
&&& r_{t} = \mu_{t} + \epsilon_{t}, \text{onde} \\
&&& \mu_{t} = \phi_{0} + \sum_{i=1}^{p}{\phi_{i}r_{t-i}} + \sum_{j=1}^{q}{\theta_{j}\epsilon_{t-j}}
\end{aligned}
$$

Já para a variância condicional, temos a equação abaixo:

$$
\begin{aligned}
&&& \epsilon_{t}=\sigma_tu_t, \text{onde} \\
&&& \sigma_{t}^{2} = \alpha_{0} + \sum_{i=1}^{m}{}\alpha_{i}\epsilon_{t-i}^{2}+\sum_{j=1}^{n}{\beta_j\sigma^{2}_{t-j}}
\end{aligned}
$$

e, $\left\{u_t \right\}_{t=1}^{t}$ é uma sequência de variáveis aleatórias independente e identicamente distribuídas (iid) com média $0$ e variância $1$, $\alpha_{0}>0$, $\alpha_{i}\geq 0$, $\beta_{j}\geq 0$ para $i>0$ e $j>0$. Além disso, $\sum_{i=1}^{max(m,n)}{(\alpha_i+\beta_i)<1}$ que garante que a variância incondicional de $\epsilon_t$ é finita. Usualmente assumimos que $\epsilon_t$ segue uma distribuição Normal, t-Student, GED ou outra de interesse (veja mais opções no capítulo 6 do livro Financial Risk Modelling and Portfolio
Optimization with R de @pfaff2016 [aqui](https://englianhu.files.wordpress.com/2017/09/financial-risk-modelling-and-portfolio-optimization-with-r-2nd-edt.pdf). Em muitos casos, usamos distribuições assimétricas para $\epsilon_t$.  

Obs: Observe que se $n=0$ a equação da variância condicional se comporta como um modelo $ARCH(m)$.

##### **PROCESSO DE ANÁLISE**

1. Especificar a equação para a média condicional ($\mu_{t}$):
    * Visualizar os dados e identificar observações fora do padrão (*outliers* ou dados faltantes) e eliminá-las.
    * Se necessário, transformar os dados para estabilizar a variância (logaritmo dos dados, variação ou retorno, por exemplo).
    * Testar se os dados são estacionários. Caso tenha raiz unitária é preciso diferenciar os dados até se tornarem estacionários. Para isso, testa-se novamente se a série diferenciada se tornou estacionária.
    * Examinar as funções de autocorrelação (FAC) e autocorrelação parcial (FACP) para determinar as ordens máximas $P$ e $Q$ para os componentes AR e MA da série estacionária (diferenciada, se necessário).
    * Estimar todas as combinações para $p$, $d$ e $q$. Aqui, $d$ será fixo e igual ao número de vezes necessárias para tornar a série original estacionáira. Se não foi preciso diferenciar a série, $d=0$.
    * Escolher dentre todos os modelos estimados no passo anterior, o modelo com menor BIC e/ou AIC.
    * Examinar se os resíduos se comportam como um ruído branco:
        * Testar autocorrelação nos resíduos: fazer o teste de Ljung-Box (verificar se você aceita a nula em para pelo menos 20 lags é uma regra de bolso usual). Visualize também a função de autocorrelação (FAC) dos resíduos. Se existem defasagens estatisticamente significante (acima da linha pontilhada), há evidências de que há autocorrelação serial.
        * Testar heterocedasticidade condicional: visualizar a função de autocorrelação (FAC) dos resíduos ao quadrado. Se existem defasagens estatisticamente significante (acima da linha pontilhada), há evidências de que haja heterocedasticidade condicional. Outra alternativa é o teste LM de @engle1982autoregressive.  
        
        Obs: Verifique nos materiais disponibilizados como selecionar os lags significativos para um modelo GARCH(m,n).
        * Verificar a distribuição de probabilidade assumida no processo de estimação: realizar teste que verifique se os resíduos se comportam de acordo com a distribuição de probabilidade adotada. Obs: Você poderá rejeitar todas as nulas.
    * Se os resíduos e resíduos ao quadrado são bem comportados (ruído branco), **obter as previsões apenas com a estimação da média condicional**. Caso haja evidências de heterocedasticidade condicional (o usual para ativos financeiros - por quê?), **avançe para o próximo passo e estime a variância condicional também**. 
2. Especificar um modelo de volatilidade e estimar **conjuntamente** as equações da média e variância condicional:
    * Examinar as funções de autocorrelação (FAC) e autocorrelação parcial (FACP) dos resíduos ao quadrado **(obtidos da estimação da média condicional)** para determinar as ordens máximas $M$ e $N$ para os componentes ARCH e GARCH, respectivamente.
    *  Escolher uma distribuição para os resíduos. Para ativos financeiros a distribuição $t$ e $t$ assimétrica são bastante utilizadas.
    * Estimar todas as combinações para $m=1,..,M$ e $n=0,...,N$ para a variância condicional juntamente com a especificação ARMA(p,q) escolhida no passo 1
    * Escolher o modelo com menor BIC e/ou AIC
3. Verificar o modelo estimado 
    * Avaliar o gráfico da função de autocorrelação dos resíduos (padronizados) estimados no passo 2. O ideal é que as defasagens não ultrapassem a linha pontilhada.
    * Avaliar se as restrições impostas sobre os parâmetros são atendidas
    * Testar se os resíduos padronizados se comportam conforme a hipótese de distribuição de probabilidade assumida no passo 2 no momento de estimar conjuntamente a média condicional e a variância condicional.
4. Visualizar os resultados
    * Gráfico da volatilidade condicional

\break 

##### **RESULTADOS**

Exemplo: A ação `PETR3.SA` da Petrobras Brasileiro S.A. negociada na BM\&FBOVESPA foi escolhida e o período de análise é de 02-01-2015 até `r format(Sys.Date(), format = "%d-%m-%Y")`. Seguindo o processo proposto, temos os seguintes resultados:

```{r, echo=FALSE, include=FALSE}
#####
##   PACOTES NECESSÁRIOS
#####

source("/cloud/project/install_and_load_packages.R")

```

1. Especificar a equação para a média condicional ($\mu_{t}$):

* O gráfico abaixo mostra a série temporal da `PETR3.SA`. É possível perceber que não existe dados faltantes na série e as mudanças abruptas nos preços está condizente com acontecimentos de mercado, tais como.... Desta forma, optou-se por não eliminar qualquer observação da série temporal

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=9, fig.height=4}
#####
##   PETROBRAS (PETR3.SA)
#####

# Dados da ação PETR3.SA desde 01/01/2017
price_day <- quantmod::getSymbols("PETR3.SA", src = "yahoo", from = '2015-01-01')
log_day_return <- na.omit(PerformanceAnalytics::Return.calculate(PETR3.SA$PETR3.SA.Close, method = "log"))

# Gráfico dos preços
plot.xts(PETR3.SA$PETR3.SA.Close, main = "Preços da PETR3", xlab = "tempo", ylab = "preços")
```

* Como sabemos, os retornos financeiros raramente apresentam tendência ou sazonalidade, com exceção eventualmente de retornos intradiários que não é o caso da série temporal em análise. Além disso, a série temporal de retornos parece ter média constante (próxima a zero). Em função dessas características, optamos por usar a série temporal dos retornos da `PETR3.SA`. O gráfico abaixo mostra a série temporal dos retornos da `PETR3.SA`:

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=9, fig.height=4}
# Gráfico dos retornos
plot.xts(log_day_return, main = "Retornos da PETR3", xlab = "tempo", ylab = "retorno")
```

* Teste de estacionariedade: a tabela abaixo apresenta os resultados do teste de raiz unitária proposto por @dickey1979distribution para cada três possíveis especificações (passeio aleatório, passeio aleatório com drift e passeio aleatório com drift e tendência). Rejeitamos a hipótese nula de presença de raiz unitária (não estacionária) ao nível de significância de 5% (p-valor < 0.05)

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.height=5, fig.width=9}
# Aqui, usamos a função adfTest do pacote fUnitRoots para testar se há raiz unitária
# na série temporal avaliada. Como observamos no gráfico da série, não há tendência
# nos dados e assim o teste verificará se a série se comporta como um passeio aleatório
# sem drift. Isto é possível por meio da opção type que tem as seguintes alternativas:
# - nc: for a regression with no intercept (constant) nor time trend (passeio aleatório)
# - c: for a regression with an intercept (constant) but no time trend (passeio aleatório com drift)
# - ct: for a regression with an intercept (constant) and a time trend (passeio aleatório com constante e tendência)
# Além disso, definimos que no máximo duas defasagens da série devem ser usadas como
# variáveis explicativas da regressão do teste. As hipóteses do teste são:
# - H0: raiz unitária (passeio aleatório)
# - H1: sem raiz unitária (não é um passeio aleatório)
unitRootnc <- fUnitRoots::adfTest(log_day_return, lags = 2, type=c("nc"))
unitRootc <- fUnitRoots::adfTest(log_day_return, lags = 2, type=c("c"))
unitRootct <- fUnitRoots::adfTest(log_day_return, lags = 2, type=c("ct"))
```


\begin{table}[h!]
\centering
\begin{tabular}{lll}
\hline
\multicolumn{1}{c}{Especificação}       & \multicolumn{1}{c}{Estatística do Teste} & \multicolumn{1}{c}{P-valor} \\ \hline
Passeio Aleatório                       & `r round(unitRootnc@test$statistic,4)`   & `r round(unitRootnc@test$p.value,4)`                            \\
Passeio Aleatório com drift             & `r round(unitRootc@test$statistic,4)`    & `r round(unitRootc@test$p.value,4)`                            \\
Passeio Aleatório com drift e tendência & `r round(unitRootct@test$statistic,4)`   & `r round(unitRootct@test$p.value,4)`                            \\ \hline
\end{tabular}
\end{table}

* Com a série temporal dos retornos (não há a necessidade de diferenciação), examinamos os gráficos da função de autocorrelação (FAC) e da função de autocorrelação parcial (FACP) abaixo. Como resultado temos que nenhuma defasagem $p$ e $q$ foi encontrada analisando a FACP e FAC, respectivamente. Assim, nosso equação da média condicional terá apenas um intercepto, ou seja, sem parâmetros para a parte AR e MA do modelo ARMA(p,q).

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.height=6, fig.width=9}
# Função de autocorrelação
acf_arma <- stats::acf(log_day_return, na.action = na.pass, plot = FALSE, lag.max = 15)

# Função de autocorrelação parcial
pacf_arma <- stats::pacf(log_day_return, na.action = na.pass, plot = FALSE, lag.max = 15)

# Gráficos 
par(mfrow=c(2,1))
plot(acf_arma, main = "", ylab = "", xlab = "Defasagem")
title("Função de Autocorrelação (FAC)", adj = 0.5, line = 1)
plot(pacf_arma, main = "", ylab = "", xlab = "Defasagem")
title("Função de Autocorrelação Parcial (FACP)", adj = 0.5, line = 1)
```

* Caso fosse encontrado valores para $p$ e $q$ diferentes dos anteriores, deveríamos estimar todas as combinações para $p$, $d$ e $q$. Aqui, $d$ será fixo e igual ao número de vezes necessárias para tornar a série original estacionária. Como não foi preciso diferenciar a série dos retornos da `PETR3.SA`, $d=0$.

```{r, echo=FALSE, warning=FALSE, message=FALSE, results = 'asis'}
# Todas as combinações possíveis de p=0 até p=max e q=0 até q=max
pars <- expand.grid(ar = 0:0, diff = 0, ma = 0:0)

# Local onde os resultados de cada modelo será armazenado
modelo <- list()

# Estimar os parâmetros dos modelos usando Máxima Verossimilhança (ML)
for (i in 1:nrow(pars)) {
  modelo[[i]] <- arima(log_day_return, order = unlist(pars[i, 1:3]), method = "ML")
}

# Obter o logaritmo da verossimilhança (valor máximo da função)
log_verossimilhanca <- list()
for (i in 1:length(modelo)) {
  log_verossimilhanca[[i]] <- modelo[[i]]$loglik
}

# Calcular o AIC
aicarma <- list()
for (i in 1:length(modelo)) {
  aicarma[[i]] <- stats::AIC(modelo[[i]])
}

# Calcular o BIC
bicarma <- list()
for (i in 1:length(modelo)) {
  bicarma[[i]] <- stats::BIC(modelo[[i]])
}

# Quantidade de parâmetros estimados por modelo
quant_parametros <- list()
for (i in 1:length(modelo)) {
  quant_parametros[[i]] <- length(modelo[[i]]$coef)+1 # +1 porque temos a variância do termo de erro 
}

# Montar a tabela com os resultados
especificacao <- paste0("arma",pars$ar,pars$diff,pars$ma)
tamanho_amostra <- rep(length(log_day_return), length(modelo))
resultado_arma <- data.frame(especificacao, ln_verossimilhanca = unlist(log_verossimilhanca),
                       quant_parametros = unlist(quant_parametros),
                       tamanho_amostra, aic = unlist(aicarma), 
                       bic = unlist(bicarma), stringsAsFactors = FALSE)

# Adicionar a tabela no PDF
tabelapdf <- xtable(resultado_arma, align = "lcccccc", digits = c(0,0,3,0,0,3,3))
print(tabelapdf, comment = FALSE)
```

* Como temos apenas um modelo estimado, não precisamos escolher o modelo com menor BIC e/ou AIC e continuaremos com o modelo $ARMA(0,0)$ e a equação da média condicional será:

$$
\begin{aligned}
&&& r_{t} = \mu_{t} + \epsilon_{t}, \text{onde} \\
&&& \mu_{t} = \phi_{0} 
\end{aligned}
$$

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Como resultado temos que o modelo escolhido tanto pelo AIC quanto pelo BIC é o ARMA(0,0)
media_condicional <- arima(log_day_return, order = c(0,0,0), method = "ML")
```

Obs: Usualmente um AR(1) ou um ARMA(1,1) são bons ajustes para uma série de retornos.

A tabela abaixo mostra o resultado para a estimação de tal equação. É possível observar que o parâmetro $\phi_{0}$ estimado não é estatisticamente significante. Isso já é esperado pela característica de uma série temporal de retornos. 

Observe a legenda mostrada para cada `*` que nos diz que quanto maior a quantidade de `*` menor o p-valor e maior a probabilidade de rejeitar a hipótese nula do teste. Quando não há `*` para um coeficiente, quer dizer que seu p-valor é maior do que os apresentados na legenda, ou seja, não conseguimos rejeitar a hipótese nula. Caso queira visualizar o p-valor para cada parâmetro, use a função `lmtest::coeftest()`.

```{r, echo = FALSE, warning = FALSE, message = FALSE, results = 'asis'}
# Parâmetros estimados. Aqui, usamos a função stargazer do pacote stargazer para 
# mostrar os resultados em um formato textual mais amigável para interpretação.
# Mais detalhes? Use help("stargazer")
stargazer::stargazer(media_condicional, type = "latex", header = FALSE, title = "Resultado Estimação modelo ARMA(0,0)")
```


* Examinar se os resíduos se comportam como um ruído branco:

No processo de definição do modelo $ARMA(p,q)$ assumimos que o termo de erro $\epsilon_{t}$ não é autocorrelacionado, ou seja, $E[\left(\epsilon_{t}-E\left(\epsilon_{t}\right)\right)\left(\epsilon_{t-1}-E\left(\epsilon_{t}\right)\right)]=E[\epsilon_{t}\epsilon_{t-1}]=0$. Como teste para verificar a validade de tal hipótese, temos abaixo o gráfico da função de autocorrelação (FAC) dos resíduos (nossa estimativa para o termo de erro) e encontramos que não há presença de autocorrelação serial dado que a grande maioria das defasagens não são estatisticamente significantes.

Obs: Faça também o teste de Ljung-Box.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=9}
# Verificar a existência de autocorrelação serial nos resíduos
acf_residuals <- acf(media_condicional$residuals, na.action = na.pass, plot = FALSE, lag.max = 20)
plot(acf_residuals, main = "", ylab = "", xlab = "Defasagem")
title("FAC dos resíduos do ARMA(0,0)", adj = 0.5, line = 1)
```

Porém, quando avaliamos o gráfico da função de autocorrelação (FAC) do quadrado dos resíduos abaixo, observamos que existem defasagens estatisticamente significante (acima da linha pontilhada). Isso confirma a presença de heterocedasticidade condicional assim como um teste LM de @engle1982autoregressive caso realizado.  

Isso não condiz com a hipótese assumida na definição do modelo $ARMA(p,q)$ de que a variância do termo de erro é contante e independente do tempo, ou seja, que $Var(\epsilon_{t}) = \sigma_{\epsilon}^{2}$.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=9}
# Verificar a existência de autocorrelação serial nos resíduos
acf_residuals_square <- acf(media_condicional$residuals^2, na.action = na.pass, plot = FALSE, lag.max = 20)
plot(acf_residuals_square, main = "", ylab = "", xlab = "Defasagem")
title("FAC do quadrado dos resíduos do ARMA(0,0)", adj = 0.5, line = 1)
```

Por fim, podemos avaliar se os resíduos do modelo estimado tem a distribuição especificada. Relembre que a definição do modelo $ARMA(p,q)$ assume que $\epsilon_{t}$ é independente e identicamente distribuído (iid) e quando executamos a estimação no R por meio da função `arima` assumimos, por default, que $\epsilon_{t}$ segue uma distribuição Normal.

A tabela abaixo mostra o resultado para os testes de [Shapiro-Wilk](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) e [Jarque Bera](https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test). A hipótese nula dos testes é que a amostra provém de **uma população com distribuição Normal** contra a hipótese alternativa que a amostra **não provém de uma população com distribuição Normal**. Como o p-valor de ambos os testes é praticamente nulo (o R arredondou), rejeitamos e hipótese nula e os resíduos obtidos da estimação da equação da média condicional não são provenientes de uma população Normal. 

Obs: Isto é muito comum e provavelmente rejeitaremos qualquer hipótese nula (quando a amostra for grande). Por quê? 

Obs 2: Veja mais detalhes no capítulo 6 do livro Financial Risk Modelling and Portfolio
Optimization with R de @pfaff2016.

Série temporais de retornos de ativos financeiros apresentam, comumente, variância condicional assimétrica (maior volatilidade na cauda inferior da distribuição do que na superior) (@hafner1998). Desta forma, não podemos assumir que apenas a estimação da equação da média condicional é o suficiente para uma série como o da `PETR3.SA` e precisamos estimar a variância condicional também.  

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Teste de Normalidade dos resíduos. As hipóteses para os dois testes são:
#  - H0: resíduos normalmente distribuídos
#  - H1: resíduos não são normalmente distribuídos
shapiro_test <- stats::shapiro.test(na.remove(media_condicional$residuals))
jarque_bera <- tseries::jarque.bera.test(na.remove(media_condicional$residuals))
```

\begin{table}[h!]
\centering
\begin{tabular}{lll}
\hline
\multicolumn{1}{c}{Teste de Normalidade}       & \multicolumn{1}{c}{Estatística do Teste} & \multicolumn{1}{c}{P-valor} \\ \hline
Shapiro                       & `r round(shapiro_test$statistic,4)`   & `r round(shapiro_test$p.value,4)`                            \\
Jarque Bera             & `r round(jarque_bera$statistic,4)`    & `r round(jarque_bera$p.value,4)`                            \\ \hline
\end{tabular}
\end{table}

2. Especificar um modelo de volatilidade e estimar **conjuntamente** as equações da média e variância condicional:
    
* Precisamos examinar as funções de autocorrelação (FAC) e autocorrelação parcial (FACP) dos resíduos ao quadrado **(obtidos da estimação da média condicional)** para determinar as ordens máximas $M$ e $N$ para os componentes ARCH e GARCH, respectivamente. Como resultado, temos abaixo tais gráficos e optamos para o exercício trabalhar com $M=3$ e $N=3$ (não é usual e geralmente não é robusto escolher valores muito grandes para $M$ e $N$ - um GARCH(1,1) é considerado o benchmark). Relembre que a escolha das ordens do GARCH é um pouco diferente da de um ARIMA. Para relembrar veja a página 277 de @delosso2012 ou a equação (34) [aqui](https://fernando-b-sabino-da-silva.rstudio.cloud/3b10a19273834e8e8ee23b472c06d1f7/file_show?path=%2Fcloud%2Fproject%2Fnotes%2F4.Serie_Temporal_Univariada%2F7.ARCH_GARCH%2Fgarch.pdf).

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.height=6, fig.width=9}
# FAC dos resíduos ao quadrado
acf_residuals_square <- acf(media_condicional$residuals^2, na.action = na.pass, plot = FALSE, lag.max = 20)

# FACP dos resíduos ao quadrado
pacf_residuals_square <- stats::pacf(media_condicional$residuals^2, plot = FALSE, na.action = na.pass, max.lag = 25)

# Gráficos 
par(mfrow=c(2,1))
plot(acf_residuals_square, main = "", ylab = "", xlab = "Defasagem")
title("FAC do quadrado dos resíduos do ARMA(0,0)", adj = 0.5, line = 1)
plot(pacf_residuals_square, main = "", ylab = "", xlab = "Defasagem")
title("FACP do quadrado dos resíduos do ARMA(0,0)", adj = 0.5, line = 1)
```

* Antes de executarmos a estimação em conjunto das equações para a média condicional e variância condicional, eu gostaria que vocês avaliassem a distribuição dos resíduos. Um exemplo pode ser visto no capítulo 6 de @pfaff2016 [aqui](https://englianhu.files.wordpress.com/2017/09/financial-risk-modelling-and-portfolio-optimization-with-r-2nd-edt.pdf) usando a função stepAIC.ghyp(). Você não deve usar esta função e sim verificar dentre as distribuições que o modelo GARCH suporta no R qual delas se ajusta melhor aos dados (veja a opção cond.dist = c("norm", "snorm", "ged", "sged", "std", "sstd", 
        "snig", "QMLE") na função garchFit()). Para tal rode o modelo com todas as variações e escolha aquele com menor BIC e/ou AIC.

Obs: O objetivo aqui é um treinamento. Na prática, é comum começar com "ideias". A t assimétrica, por exemplo, atende usualmente os propósitos de acordo com os fatos estilizados. O que eu pedi acima é um procedimento data-driven, mas é razoável "pensar" e usar as informações disponíveis para a tomada de decisões.

\break 

* Uma vez que definimos as ordens máximas para $M$ e $N$ bem como uma distribuição de probabilidade que melhor "imita" a distribuição dos resíduos, podemos estimar todas as combinações para $m=1,..,M$ e $n=0,...,N$ para a variância condicional juntamente com a especificação ARMA(p,q) escolhida no passo 1. Como é possível observar na tabela abaixo o modelo com menor AIC e BIC é o $ARMA(0,0)-GARCH(1,1)$.

Obs: Abaixo adotamos uma distribuição $t$ de Student para os resíduos. Eu não fiz os testes devidos neste exemplo.

```{r, echo=FALSE, warning=FALSE, message=FALSE, results = 'asis'}
# Todas as combinações possíveis de m=1 até m=max e n=0 até n=max
pars_arma_garch <- expand.grid(m = 1:3, n = 0:3)

# Local onde os resultados de cada modelo será armazenado
modelo_arma_garch <- list()

# Especificação arma encontrada na estimação da média condicional
arma_set <- "~arma(0,0)"

# Distribuição de probabilidade assumida para o termo de erro da média condicional 
# - norm: normal, std: t-student, snorm: normal assimétrica, sstd: t-student assimétrica
arma_residuals_dist <- "std"

# Definição se o processo estimará parâmetros de assimetria e curtose para a distribuição
include.skew = FALSE
include.shape = TRUE

# Estimar os parâmetros dos modelos usando Máxima Verossimilhança (ML)
for (i in 1:nrow(pars_arma_garch)) {
  modelo_arma_garch[[i]] <- fGarch::garchFit(as.formula(paste0(arma_set,"+","garch(",pars_arma_garch[i,1],",",pars_arma_garch[i,2], ")")),
                                            data = log_day_return, trace = FALSE, cond.dist = arma_residuals_dist,
                                            include.skew = include.skew, include.shape = include.shape) 
}

# Obter o logaritmo da verossimilhança (valor máximo da função)
log_verossimilhanca_arma_garch <- list()
for (i in 1:length(modelo_arma_garch)) {
  log_verossimilhanca_arma_garch[[i]] <- modelo_arma_garch[[i]]@fit$llh
}

# Calcular o AIC
aicarma_garch <- list()
for (i in 1:length(modelo_arma_garch)) {
  aicarma_garch[[i]] <- modelo_arma_garch[[i]]@fit$ics[1]
}

# Calcular o BIC
bicarma_garch <- list()
for (i in 1:length(modelo_arma_garch)) {
  bicarma_garch[[i]] <- modelo_arma_garch[[i]]@fit$ics[2]
}

# Quantidade de parâmetros estimados por modelo
quant_paramentros_arma_garch <- list()
for (i in 1:length(modelo_arma_garch)) {
  quant_paramentros_arma_garch[[i]] <- length(modelo_arma_garch[[i]]@fit$coef)
}

# Montar a tabela com os resultados
especificacao <- paste0(arma_set,"-","garch",pars_arma_garch$m,pars_arma_garch$n)
tamanho_amostra <- rep(length(log_day_return), length(modelo_arma_garch))
resultado_arma_garch <- data.frame(especificacao, ln_verossimilhanca = unlist(log_verossimilhanca_arma_garch),
                       quant_paramentros = unlist(quant_paramentros_arma_garch),
                       tamanho_amostra, aic = unlist(aicarma_garch), bic = unlist(bicarma_garch),
                       stringsAsFactors = FALSE, row.names = NULL)

# Adicionar a tabela no PDF
tabelapdf_arma_garch <- xtable(resultado_arma_garch, align = "lcccccc", digits = c(0,0,3,0,0,3,3))
print(tabelapdf_arma_garch, comment = FALSE)

```

Na tabela 2 temos os resultados dos parâmetros estimados para o modelo escolhido. Observe que temos uma estimativa para a curtose da distribuição assumida (parâmetro `shape`). Tal estimativa está bem próxima do valor obtido quando calculamos a curtose da série temporal dos resíduos obtidos na estimação da média condicional apenas. 

```{r, echo = FALSE, warning = FALSE, message = FALSE, results = 'asis'}
# Parâmetros estimados. Aqui, usamos a função stargazer do pacote stargazer para 
# mostrar os resultados em um formato textual mais amigável para interpretação.
# Mais detalhes? Use help("stargazer")
stargazer::stargazer(modelo_arma_garch[[4]], type = "latex", header = FALSE, title = "Resultado Estimação modelo ARMA(0,0)-GARCH(1,1)")
```

3. Verificar o modelo estimado 

* Uma vez estimado o modelo, vamos avaliar se ele continua com "heterocedasticidade condicional". Se nosso modelo realmente captou a heterocedasticidade condicional, não deveríamos ter defasagens no gráfico da FAC dos resíduos ao quadrado ultrapassando a linha pontilhada (faça também o teste de Ljung-Box). O gráfico abaixo confima que não há mais evidências de heterocedasticidade condicional nos resíduos do modelo $ARMA(0,0)-GARCH(1,1)$. 

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.height=6, fig.width=9}

# FAC dos resíduos ao quadrado
acf_residuals_square_arma_garch <- acf(fGarch::residuals(modelo_arma_garch[[4]], standardize = TRUE)^2, na.action = na.pass, plot = FALSE, lag.max = 20)

# Gráficos 
plot(acf_residuals_square_arma_garch, main = "", ylab = "", xlab = "Defasagem")
title("FAC do quadrado dos resíduos do ARMA(0,0)-GARCH(1,1)", adj = 0.5, line = 1)
```

* Como é possível observar nos resultados obtidos para os parâmetros do modelo. As restrições sobre os mesmos são mantidas, pois os valores obtidos para $\alpha_0$, $\alpha_1$ e $\beta_1$ condizem com as restrições.


4. Visualizar os resultados

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.height=4, fig.width=9}
plot(modelo_arma_garch[[4]], which = 3)
```

5. Obter as previsões (veja a função predict da library fGarch, por exemplo). Queremos também saber qual a acurácia da previsão estimada para o seu modelo (avaliada no conjunto de validação). Procure pela função accuracy do R. Veja exemplos [aqui](https://rpubs.com/RatherBit/90267) e [aqui](https://www.pedronl.com/post/previsao-de-series-temporais-com-o-r/).

Obs: Por medidas de acurácia eu estou me referindo as medidas ME, RMSE, MAE, MPE, MAPE, MASE, ACF1 e Theil's U. Elas não foram calculadas no exemplo acima.

```{r, warning=FALSE, message=FALSE, echo=FALSE, results='asis'}
# usar a função predict do pacote fGarch para fazer a previsão 3 passos à frente usando o modelo escolhido anteriormente
forecast <- fGarch::predict(modelo_arma_garch[[4]], n.ahead = 3)
stargazer::stargazer(forecast, summary = FALSE, type = "latex", header = FALSE)
```

Obs 2: Nós queremos um modelo que faça boas previsões fora da amostra (out of sample). Ou no conjunto de teste como se diz em Machine Learning. Um modelo que explica a variação nos dados dentro da amostra (in sample, no conjunto de treinamento), mas não obtém bons resultados para dados externos é um modelo referido na literatura como sobreajustado (overfitted).

Você pode pensar de que esse é motivo pelo qual nós dividimos a amostra em dados de treinamento e dados de teste. Correto, mas não é uma boa ideia avaliar repetidamente o nosso conjunto de teste, pois o modelo pode se tornar sobreajustado apenas para aquele pedaço. Para resolver isto costuma-se usar “data splitting” em mais partes. Um método utilizado para tal é conhecido por validação cruzada (cross validation). Ele testa o modelo usando os próprios dados de treinamento.

Seja, por exemplo, K = 5, isto é, imagine que iremos dividir os dados de treinamento em cinco porções iguais. Um dos cinco pedaços é colocado de lado (como um conjunto de dados de mini-teste) e, em seguida, o modelo é treinado usando as outras 4 porções. Depois disso, as previsões são feitas para o pedaço retido, e na sequência o processo é repetido para cada uma das 5 porções e a média das previsões produzidas a partir das iterações do modelo é calculada. Isto nos dá uma compreensão de quão bem o modelo prevê dados externos!

Amostragem é possivelmente o aspecto mais importante de qualquer projeto quantitativo financeiro. Devemos usar prioritariamente dados históricos para prever o futuro. Aqui preciso fazer uma observação: amostragem via backtesting (ou seja, usando parte dos dados para treinar um modelo e outra parte para testá-lo) é insuficiente! A validação cruzada também é! Por favor, veja [aqui](https://www.ams.org/notices/201405/rnoti-p458.pdf) e [aqui](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3104816) para um tratamento mais aprofundado dessa questão!

##### **REFERÊNCIAS**



